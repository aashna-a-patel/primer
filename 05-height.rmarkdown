# Height

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(gt)
library(tidyverse)
library(primer.data)
library(skimr)
library(tidymodels)
library(equatiomatic)
library(broom)
```

@sec-models used a logistic regression model—an appropriate choice when the outcome has just two possible values. In this chapter, we’ll use a linear regression model because our outcome variable, height, is continuous.

We don’t build models just for fun—though it is fun. We build models because the world presents us with decisions. Should we choose X or Y? Action A, B, or C? When faced with choices, we create models to better understand how the world works and to make smarter, more informed decisions.

Of course, the real world is messy. Real decisions come with complexity and context we can’t fully capture here. So, we simplify. In this chapter, we’ll model adult height using a linear regression and explore two key questions:

Scenario 1 (Predictive): 
Imagine that you're in charge of ordering uniforms for next year’s Marine Corps bootcamp recruits. There are many decisions to make: the cost of different designs, the number of male and female recruits, the distributions of heights and weights, and so on.

How does average height differ between men and women?

---

Scenario 2 (Causal):
Imagine that you're advising a school district that’s considering implementing daily physical education (PE) for younger students. There are many decisions to make: how much time to devote to PE, whether it will affect academic schedules, and whether it will improve long-term health outcomes. NHANES includes data on physical activity levels and self-reported height.

Does higher physical activity during youth lead to greater adult height?

> The world confronts us. Make decisions we must.

Data science is ultimately a moral act, so we will use the four [Cardinal Virtues](https://en.wikipedia.org/wiki/Cardinal_virtues) --- Wisdom, Justice, Courage and Temperance --- to guide our approach.  

## Wisdom

```{r}
#| echo: false
knitr::include_graphics("two-parameters/images/learn_the ways.gif")
```

Wisdom begins with the Preceptor Table: the ideal data needed to answer our questions. Next, we examine the data we actually have. Using the concept of *validity*, we assess whether the data we possess are sufficiently similar to the ideal data to provide meaningful insights.

### Preceptor Table

Which rows and columns of data do you need such that, if you had them all, the calculation of the quantity of interest would be trivial? The steps we usually take to construct the Preceptor Table include:

**Scenario 1:** How does average height differ between men and women?

* *Units*: All adults in the world, one row per person.
* *Outcome*: Individual height. While the question concerns average height, modeling individual heights allows calculation of group averages.
* *Treatment*: None.
* *Causal or predictive model*: Predictive. We aim to estimate differences, not causal effects.
* *Covariates*: Sex must be included to differentiate men and women.
* *Moment in Time*: Present—heights measured “now.”

The Preceptor Table here would have about 8 billion rows (one per adult) and columns for sex and height.

---

**Scenario 2:** Does participation in a youth physical activity program cause an increase in adult height?

* *Units*: All adults, regardless of participation status.
* *Outcome*: Adult height.
* *Treatment*: Participation in the youth physical activity program.
* *Causal or predictive model*: Causal. The goal is to estimate the effect of the program on height.
* *Covariates*: Sex, socioeconomic status, nutrition, and age at treatment to control for confounding.
* *Moment in Time*: Treatment in the past; height measured now.

This Preceptor Table would also have many rows—one per adult—with columns for treatment, height, sex, and relevant covariates.

Here are some rows from our Preceptor Tables: 

Scenario 1:
```{r}
#| echo: false
scenario1_table <- tibble(
  ID = c("Adult 1", "Adult 2", "...", "Adult N"),
  height = c("165", "178", "...", "170"),
  sex = c("Female", "Male", "...", "Female")
) |>
  gt() |>
  cols_label(
    ID = md("ID"),
    height = md("Height (cm)"),
    sex = md("Sex")
  )

scenario1_table
```

This table would have about 8 billion rows—one per adult. With it, calculating average height by sex would be straightforward, no modeling needed. 

---

Scenario 2:

```{r}
#| echo: false
scenario2_table <- tibble(
  ID = c("Adult 1", "Adult 2", "...", "Adult N"),
  treatment = c("Yes", "No", "...", "No"),
  height = c("172", "160", "...", "168"),
  sex = c("Male", "Female", "...", "Male"),
  socioeconomic_status = c("Low", "High", "...", "Medium")
) |>
  gt() |>
  cols_label(
    ID = md("ID"),
    treatment = md("Program Participation"),
    height = md("Height (cm)"),
    sex = md("Sex"),
    socioeconomic_status = md("Socioeconomic Status")
  )

scenario2_table
```

This table would include all adults with their treatment status and covariates. If complete, we could directly assess the program’s effect on height.

But what does our actual data look like?

Like all aspects of a data science problem, the Preceptor Table evolves as we continue our work. 

### EDA

Consider the `nhanes` dataset from the National Health and Nutrition Examination Survey conducted by the CDC between 2009 and 2011.

NHANES is a voluntary survey that oversamples groups such as older adults and minorities, which can make the raw data unbalanced for the overall population. To address this, sampling weights are applied for accurate representation.

```{r}
#| code-fold: false
#| warning: false
glimpse(nhanes)
```

The dataset contains 10,000 entries from the 2009 survey, each representing an individual’s health and demographic information. Some values, like `pulse` and `pregnancies`, have missing data, and repeated values likely reflect individuals from the same household.

The dataset includes `r ncol(nhanes)` variables covering physical attributes such as weight and height. For now, we focus on three: `age`, `sex`, and `height`.

```{r}
#| code-fold: false
nhanes |> 
  select(age, sex, height)
```

This subset displays `age`, `sex`, and `height` for 10,000 individuals, including both children and adults. Repeated rows may indicate household clustering or duplicate observations.

Examine a random sample:

```{r}
#| code-fold: false
nhanes |> 
  select(age, sex, height) |> 
  slice_sample(n = 5)
```

Always examine your data carefully. Are there any missing (`NA`) values? Do the column types make sense—for example, why is `age` stored as an integer rather than a double? Are there more females than males?

**You can never look at your data too closely.**

Alongside `glimpse()`, the `skim()` function from the **skimr** package provides a helpful overview, including summary statistics for each variable.

```{r}
#| code-fold: false
nhanes |> 
  select(age, sex, height) |> 
  skim()
```

Interesting—there are `r sum(is.na(nhanes$height))` missing values in the `height` column. This isn't immediately obvious from `glimpse()`. To remove these, we’ll use `drop_na()`, which deletes any rows with missing values. To make the statistical inference more visually meaningful, we’ll work with a random sample of 50 observations. In practice, of course, we would use the full dataset.

```{r}
#| code-fold: false

# We use set.seed() to ensure that anyone running this code will get the same
# set of observations in their tibble. See ?set.seed for details.

set.seed(10)

x <- nhanes |> 
  filter(age >= 18) |> 
  select(height, sex) |> 
  drop_na() |> 
  slice_sample(n = 50)
```

Let's plot this data using `geom_histogram()`. 

```{r}
x |> 
  ggplot(aes(x = height)) +
  geom_histogram(bins = 50, fill = "#69b3a2", color = "white") +
  labs(
    title = "Adult Heights (Sample of 50) from NHANES 2010",
    subtitle = "Subset of 50 adults aged 18+ from NHANES, average height ≈ 162 cm",
    x = "Height (cm)",
    y = "Count",
    caption = "Source: National Health and Nutrition Examination Survey"
  ) +
  theme_minimal(base_size = 14)
```

Can data from a sample of adult Americans collected over a decade ago still help us answer our questions—even approximately? Only if the assumption of *validity* holds.

## Justice

```{r}
#| echo: false
knitr::include_graphics("other/images/Justice.jpg")
```

> Justice concerns the Population Table and the four key assumptions which underlie it: validity, stability, representativeness, and unconfoundedness.

### Validity

**Validity** refers to whether variables across different datasets represent the same underlying concept. For example, does `height` in the Preceptor Table mean the same as `height` in NHANES? Most likely—but we must consider potential differences, such as units (centimeters vs. inches) or measurement conditions (with or without shoes). These discrepancies could affect alignment. Still, the NHANES 2010 height data is a reasonable proxy for current adult height, allowing us to treat both datasets as drawn from the same population. The same reasoning applies to the `sex` variable.

Because validity holds, we can merge the Preceptor Table with our actual data to form the Population Table.

### Stability

**Stability** means that the relationship between variables in the Population Table remains consistent across all three sources: the actual data, the Preceptor Table, and the broader population from which both are drawn.

### Representativeness

**Representativeness** concerns two key relationships among the rows in the Population Table: first, how well the data represents the broader population; and second, how well the population reflects the Preceptor Table.

In our case, the population includes all adults worldwide. Is U.S. data from 2010 representative of that global population? Not exactly. People in the U.S. differ in systematic ways—both biologically and culturally—from those in other parts of the world.

### Unconfoundedness

**Unconfoundedness** means that, once we account for pre-treatment covariates, the assignment of treatment is independent of the potential outcomes. In other words, there are no hidden biases linking who gets the treatment to what their outcomes would have been. A model is **confounded** when this assumption does not hold.

### Population

The **population** is not the same as the dataset we have—participants from the CDC’s NHANES survey conducted between 2009 and 2011. That’s simply our **data**. Nor is the population the set of individuals we *wish* we had data on—those make up the **Preceptor Table**. The population is the broader set of individuals that includes both the people we’ve observed and those we want to observe. Typically, this population is much larger than either group.

There's almost always a time component to consider. Our data comes from the past, but the questions we ask are often about the present or even the future. For instance, in **Scenario 1**, we want to estimate average adult height *today*, not just in 2009–2011. And in **Scenario 2**, we may want to assess the impact of a treatment across broader or more current populations than the original data covers.

Can we generalize from the data we have to the population we care about? That depends. U.S. adults from over a decade ago are not identical to today’s global adult population. Cultural, environmental, and biological differences matter. Still, this data may be the best we have—and often, using imperfect data is better than using none at all.

But not always. If the data is irrelevant to the question—for example, trying to estimate the median height of fifth-grade girls in Tokyo from this sample—it may do more harm than good. **Wisdom** lies in knowing when your data is “good enough” and when to seek better information—or rely on other forms of judgment.

### The Population Table

> The **Population Table** includes one row for each unit–time combination in the full population from which both the Preceptor Table and the observed data are drawn.

It brings together three types of rows:

* **Preceptor Table rows** represent the ideal data we *wish* we had to answer our question. These rows include covariates like `sex`, but no outcomes.
* **Actual data rows** represent what we *do* have—both covariates and outcomes. For example, in our dataset, these rows may come from adults surveyed in 2009–2011, with complete entries for `sex`, `height`, and `year`.
* **Other rows** represent individuals in the broader population for whom we have no data. These rows are empty except for structural identifiers like ID and time. While we don’t truly know those values, we define them conceptually to complete the population structure.

The Population Table allows us to frame our question in terms of what we ideally want to know, what we actually observe, and where gaps in data remain. This framework applies whether we’re estimating average height today (Scenario 1) or assessing a treatment effect (Scenario 2).

Scenario 1:
```{r}
#| echo: false
tibble(
  ID = c("P1", "P2", "...", "D1", "D2", "D3", "...", "O1", "O2", "O3"),
  source = c("Preceptor", "Preceptor", "...", 
             "Actual", "Actual", "Actual", "...", 
             "Other", "Other", "Other"),
  year = c("2024", "2024", "...", 
           "2009", "2010", "2011", "...", 
           "?", "?", "?"),
  sex = c("Male", "Female", "...", 
          "Male", "Female", "Male", "...", 
          "?", "?", "?"),
  height = c("?", "?", "...", 
             "180", "160", "168", "...", 
             "?", "?", "?")
) |>
  gt() |>
  tab_header(title = "Population Table – Height Prediction") |>
  cols_label(
    ID = md("ID"),
    source = md("Source"),
    year = md("Year"),
    sex = md("Sex"),
    height = md("Height")
  ) |>
  tab_spanner(label = "Covariates", columns = c(sex)) |>
  tab_spanner(label = "Outcome", columns = c(height)) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything())
```

---

Scenario 2:

```{r}
#| echo: false
tibble(
  ID = c("P1", "P2", "...", "D1", "D2", "D3", "...", "O1", "O2", "O3"),
  source = c("Preceptor", "Preceptor", "...", 
             "Actual", "Actual", "Actual", "...", 
             "Other", "Other", "Other"),
  year = c("2024", "2024", "...", 
           "2009", "2010", "2011", "...", 
           "?", "?", "?"),
  treatment = c("Yes", "No", "...", 
                "Yes", "No", "Yes", "...", 
                "?", "?", "?"),
  height = c("?", "?", "...", 
             "176", "162", "170", "...", 
             "?", "?", "?")
) |>
  gt() |>
  tab_header(title = "Population Table – Treatment Effect on Height") |>
  cols_label(
    ID = md("ID"),
    source = md("Source"),
    year = md("Year"),
    treatment = md("Treatment"),
    height = md("Height")
  ) |>
  tab_spanner(label = "Covariates", columns = c(treatment)) |>
  tab_spanner(label = "Outcome", columns = c(height)) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything())
```

## Courage

```{r}
#| echo: false
knitr::include_graphics("other/images/Courage.jpg")
```

In data science, *we deal with words, math, and code, but the most important of these is code.* We need Courage to create the model, to take the leap of faith that we can make our ideas real. 

**Justice** helped us define the Population Table—the framework of the world we’re trying to understand. **Courage** guides the next step: choosing the data-generating process. We begin by specifying a mathematical relationship between the outcome we care about and the variables we observe. We explore different modeling options, decide which covariates to include, estimate unknown parameters, and check whether our models fit the data well. Rather than rely on traditional hypothesis testing, we focus on clarity and reasoned judgment to select a final model.

We use a simple linear model:

$$
y_i = \beta_0 + \beta_1 \cdot \text{sex}_\text{Male} + \epsilon_i
$$

with $\epsilon_i \sim N(0, \sigma^2)$. Here, $y_i$ represents the height of adult $i$, and $\epsilon_i$ is the *error term*—the difference between the actual height of individual $i$ and the height predicted by the model. We assume the errors are normally distributed with mean 0 and standard deviation $\sigma$.

::: callout-note
**Model Parameters**

- This model includes three unknown parameters: $\beta_0$, $\beta_1$, and $\sigma$. While we can never know their exact values—*perfection lies only in God's own R code*—we can estimate their *posterior distributions* using the Bayesian methods introduced in Chapters @sec-probability and @sec-models.

- As George Box famously said, *"All models are wrong, but some are useful."* This model is certainly wrong, but we hope it is useful.

- The parameter we care most about is $\mu$, the *population* average height of adult men. This is **not** the same as the sample average, which we can compute directly with `mean(x$height)`. Instead, $\mu$ refers to the true mean in the population we're trying to understand.

- What exactly is that population? If we are walking around Copenhagen, the relevant population might be the set of adult men we could meet today—not U.S. men in 2010. Still, we might assume both sets are drawn from the same underlying population. Whether this assumption is reasonable is a question of *validity*.

- $\sigma$ represents the standard deviation of the residuals—how much individual heights vary from the group average. While we estimate its posterior distribution like the others, $\sigma$ is considered a *nuisance* or *auxiliary* parameter: useful for inference, but not of direct interest.
:::

### Models

Let's estimate a simple version of the model using the **tidymodels** package.

```{r}
#| code-fold: false
#| message: false
library(tidymodels)
```

Because we are estimating a linear model, we begin with:

```{r}
#| echo: true
#| message: false

fit_1 <- linear_reg() |> 
  set_engine("lm") |> 
  fit(height ~ sex, data = x)
```

The formula `height ~ sex` defines a simple linear regression model where we predict height based on sex. This corresponds to the mathematical model:

$$
y_i = \beta_0 + \beta_1 \cdot \text{sex}_{\text{Male}} + \epsilon_i
$$

Here, $y_i$ represents the height of individual $i$, and $\epsilon_i$ is the error term, assumed to follow a normal distribution: $\epsilon_i \sim N(0, \sigma^2)$. The intercept $\beta_0$ estimates the average height for the reference group (females), while $\beta_1$ captures the average height difference between males and females.

```{r}
#| echo: true
fit_1
```

Printing the model shows the fitted coefficients. In R, the `(Intercept)` term corresponds to $\beta_0$, and the coefficient for `sexMale` corresponds to $\beta_1$, which estimates the average increase in height for males relative to females.

To view the model results more clearly, we can use the `tidy()` function from the **broom** package:

```{r}
#| code-fold: false
#| message: false
#| echo: true
library(broom)

tidy(fit_1, conf.int = TRUE)
```

* The intercept (around 165 cm) gives the estimated average height for females.
* The `sexMale` coefficient (approximately 9.56 cm) indicates that males are, on average, 9.56 cm taller than females in our sample.
* The 95% confidence interval for this difference helps quantify the uncertainty around that estimate.

To double-check, let’s compute the actual average heights by sex:

```{r}
#| echo: true
x |> 
  summarise(avg_height = mean(height), .by = sex)
```

These averages confirm the model’s results: females average about 165 cm, and males about 174 cm.

The standard error provides a measure of how much the estimated average might vary if we took another sample. We can approximate it manually as:

```{r}
#| echo: true
sd(x$height) / sqrt(nrow(x))
```

This gives us a sense of the model's precision. Confidence intervals are based on this standard error, calculated as:

$$
\text{Estimate} \pm 2 \times \text{Standard Error}
$$

While we estimate $\sigma$, the standard deviation of the residuals, it's considered a **nuisance parameter**—necessary for modeling but not of primary interest. Our focus is on $\mu$, the population mean height for females, and the difference captured by $\beta_1$.


### Data Generating Mechanism

We model `height`, a continuous variable measured in centimeters, using a simple linear model with a constant term. Based on available data, the average adult male height in the U.S. is approximately 176 cm. The model can be expressed as:

$$
\text{height}_i = 176 + \epsilon_i
$$

where the error term $\epsilon_i \sim N(0, 0.75^2)$ accounts for individual variability around the mean.

---

### Posterior Predictive Check

Before accepting `fit_1` as our data-generating mechanism, we should evaluate its adequacy using a posterior predictive check.

```{r}
## tidymodels::pp_check(fit_1)
```

This diagnostic compares the observed data, $y$, with replicated data sets, $y_{\text{rep}}$, generated from the fitted model. In this case, the observed and replicated values align closely, suggesting that `fit_1` captures the overall distribution well enough to proceed.

However, the fit is not perfect. The observed data is slightly more peaked, and there are minor deviations in the tails—particularly around 200 cm. While it is theoretically possible to refine the model to better match these characteristics, such refinements may complicate the model unnecessarily. For our current purposes, `fit_1` serves as an acceptable and reasonable approximation of the true data-generating process.

